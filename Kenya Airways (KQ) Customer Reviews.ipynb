{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNpZkgLrpWiF"
      },
      "source": [
        "# **Kenya Airways & Industry Airline Customer Reviews Analysis Notebook**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "![Kenya Airways Image](https://www.kindpng.com/picc/m/337-3373993_kenya-airways-hd-png-download.png)\n",
        "\n",
        "> ## **Introduction**   \n",
        "> Kenya Airways receives airline reviews from trip advisors from both local and international travellers. Their customer service team would like to extract insights from their customer reviews on TripAdvisor and conduct competitor analysis of the top 10 airlines from Skytrax Ranking to discover their competitive edge and where they fall short.\n",
        "\n",
        "> However, they need help analyzing reviews due to the large volume of customer reviews they have to go through manually. It's time-consuming and resource-intensive. Additionally, there’s a challenge in identifying common trends and themes in customer feedback, considering customers provide review feedback on a wide range of topics, e.g. quality of food to their in-flight experiences.\n",
        "\n",
        "> In this notebook we will be using text mining and sentiment analysis to process and analyze customer reviews to help Kenya Airways overcome these challenges through Data Science & Analytics. This would allow the airline to quickly and efficiently gain insights from the data and identify common issues and trends in customer feedback. The airline could then use this information to improve its products and services and provide better support to its customers.\n",
        "\n",
        "> ## **Dataset Source**   \n",
        "> To meet the objectives of the analysis we've extracted Airline customer reviews feedback from TripAdvisor for Kenya Airways and the top 10 leading airlines in Africa by SkyTrack Ranking. This datasets will help us analyze reviews both at organization level (Kenya Airways) and how it compares to Industry (9 other airlines).\n",
        "\n",
        "Datasets can be accessed through github repository via [This Link](https://github.com/billyotieno/analytics-datasets/tree/main/Transport%20Services/Airlines/african-airlines-reviews-dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGe2BD5ZEY5N",
        "outputId": "03a31377-2b63-4d5d-c883-261fbe6a7834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Feb 22 20:32:09 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P0    29W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU connectivity\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKjVMtQDEd98",
        "outputId": "e47f4ffc-3c6f-41d9-e508-e8e8bc0ba00b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ],
      "source": [
        "# Check Hi RAM Allocation\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM0RD92SwuJF"
      },
      "source": [
        "# **Table of Contents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "sJ96jWl7n0mu"
      },
      "source": [
        ">[Kenya Airways & Industry Airline Customer Reviews Analysis Notebook](#scrollTo=jNpZkgLrpWiF)\n",
        "\n",
        ">>[Introduction](#scrollTo=jNpZkgLrpWiF)\n",
        "\n",
        ">>[Dataset Source](#scrollTo=jNpZkgLrpWiF)\n",
        "\n",
        ">[Table of Contents](#scrollTo=DM0RD92SwuJF)\n",
        "\n",
        ">>[Setting up and Installing Required Libraries](#scrollTo=5yvGfaTlygLb)\n",
        "\n",
        ">>[Sourcing Data from the Github Respository](#scrollTo=j4rYvvQ61Ps7)\n",
        "\n",
        ">>[Importing Required Libraries](#scrollTo=Yuqv8THG2ZXG)\n",
        "\n",
        ">>[Loading Data into DataFrames](#scrollTo=sII_fRkJpVcT)\n",
        "\n",
        ">>[Initial Data Exploration](#scrollTo=L6LOVnaX0mhI)\n",
        "\n",
        ">>>[Renaming Columns to Clear Columns](#scrollTo=RdNUoUxq9XNA)\n",
        "\n",
        ">>>[Checking Dataset Shape](#scrollTo=aB5iN376RWDo)\n",
        "\n",
        ">>>[Checking DataTypes](#scrollTo=8WxPPNl0RcRH)\n",
        "\n",
        ">>>[Checking for Missing Values](#scrollTo=8oOLLEXfRfj8)\n",
        "\n",
        ">>>[Dataset Description](#scrollTo=pYf9YG2GRjIL)\n",
        "\n",
        ">>>[Initial Data Cleaning: Overlapped Text](#scrollTo=n6Peyes6Rowq)\n",
        "\n",
        ">>[Data Exploration: Focused on Non-Review Columns](#scrollTo=bO-JbEUMR30e)\n",
        "\n",
        ">>>[Total Number of Reviews by Airlines](#scrollTo=xjZz7X0gRw9q)\n",
        "\n",
        ">>>[Flight Types or Regions Travelled by Reviewers for Each Airline](#scrollTo=kZe6gvL1SO-T)\n",
        "\n",
        ">>>[Distribution of Ratings (1 -5)](#scrollTo=dVmmWr8LSjsC)\n",
        "\n",
        ">>>[Average Rating Across the Airlines for the Various Travel Classes](#scrollTo=aFC4ts_yTchJ)\n",
        "\n",
        ">>>[Data Cleaning: Correcting Travel Month Column](#scrollTo=znqKrX7ZTtpb)\n",
        "\n",
        ">>>[Exploring Review Ratings by Airline and Flight Travel Class](#scrollTo=FzY5o-D7T2cU)\n",
        "\n",
        ">>>[Exploring Review Ratings by Airlines across Regions](#scrollTo=ET3DkYQyt0rf)\n",
        "\n",
        ">>>[Breakdown of Airlines by Respective Travel Classes](#scrollTo=wvNVWzd8uk0Z)\n",
        "\n",
        ">>[Data Exploration: Focused on Review Text](#scrollTo=R5Fe-nXgZ3G7)\n",
        "\n",
        ">>>[Checking for NaNs in Extracted Review Columns](#scrollTo=3KTia-Ad8L6w)\n",
        "\n",
        ">>>[Features distributions into Boolean, Categorical and Numerical types](#scrollTo=YvyCIsjiAseg)\n",
        "\n",
        ">>>[Plotting the correlation matrix for the features](#scrollTo=6V63NJ4jBlXl)\n",
        "\n",
        ">>[Data Quality Summary](#scrollTo=1ZosZNZ4mAvp)\n",
        "\n",
        ">>[Data Preparation](#scrollTo=MqaDRPCsq64H)\n",
        "\n",
        ">>>[1. Merging the two Datasets - Text Profiled & Non-Review Dataset](#scrollTo=zDXZ9KbArND8)\n",
        "\n",
        ">>>[2. Removing Duplicate Rows](#scrollTo=clfUWwL0Yxqm)\n",
        "\n",
        ">>>[3. Removing Redundant / Unrequired Columns - Select Data](#scrollTo=tYsbet3FXjwg)\n",
        "\n",
        ">>>[4. Cleaning Travel Month & Year - Select Data](#scrollTo=FDt1YXyDc74S)\n",
        "\n",
        ">>>[5. DataType Conversion](#scrollTo=gpXMSwBvsqE0)\n",
        "\n",
        ">>>[6. Review Sentiment - New Column from Rating Scores](#scrollTo=0CBTTd-Da8RB)\n",
        "\n",
        ">>[Data Preparation - Text Pre-processing for Reviews](#scrollTo=E8DIrwWyCFaj)\n",
        "\n",
        ">>[Exploratory Data Analysis / Modelling](#scrollTo=GIFr98omeGvK)\n",
        "\n",
        ">>>[Building a Quick Sentiment Classifier using CountVectorizer on Airline Reviews](#scrollTo=GIFr98omeGvK)\n",
        "\n",
        ">>>[POS - Review Text Parts of Speech Analysis](#scrollTo=M9Ow5ul0ZdO9)\n",
        "\n",
        ">>>[Review Text - Bigram Analysis](#scrollTo=dh_oAFVKgT6v)\n",
        "\n",
        ">>>[Review Text - Trigram Analysis](#scrollTo=1Krld8DlgYCR)\n",
        "\n",
        ">>[Sentiment Analysis Modelling](#scrollTo=Vnr-y3BToI7t)\n",
        "\n",
        ">>>[Using CountVectorizer & LogisticsRegression](#scrollTo=-_VuyN_IoYq9)\n",
        "\n",
        ">>>[Using TfidfVectorizer & MultinomialNB Model](#scrollTo=BOtBTxWLkRhN)\n",
        "\n",
        ">>>[Using CountVectorizer & XGBoost Model](#scrollTo=jxBS5eJ7t_ng)\n",
        "\n",
        ">>[Topic Modelling](#scrollTo=G2zOuPa3oma5)\n",
        "\n",
        ">>>[Displaying and Evaluating Topics](#scrollTo=1hTR__MCslGq)\n",
        "\n",
        ">>>[Topic Modelling with BertTopic](#scrollTo=wAIelvHoJhOr)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yvGfaTlygLb"
      },
      "source": [
        "## **Setting up and Installing Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVb_qF2RXIQK",
        "outputId": "4d8d1c93-9d0e-4230-de55-016f0bc34d84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Installing required libraries (-q quiet installing all libraries)\n",
        "! pip install -q pandas pandera numpy matplotlib seaborn textblob dask missingno wordcloud pyldavis\n",
        "! pip install -q fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZ6tj957IwSO",
        "outputId": "d84e08b9-052d-4a2a-8e80-6d628eca7197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 20:33:35--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 131266198 (125M) [application/octet-stream]\n",
            "Saving to: ‘lid.176.bin’\n",
            "\n",
            "lid.176.bin         100%[===================>] 125.18M  24.2MB/s    in 6.3s    \n",
            "\n",
            "2023-02-22 20:33:42 (19.9 MB/s) - ‘lid.176.bin’ saved [131266198/131266198]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8R76pIM-5NM",
        "outputId": "306e9fcc-fa7f-4153-acec-31406b2372c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.1/22.1 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.7/102.7 KB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.5/679.5 KB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 KB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ydata-profiling (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install pandas profiling - required for initial exploration\n",
        "!pip install -q https://github.com/pandas-profiling/pandas-profiling/archive/master.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICXczVQ0D5Od"
      },
      "outputs": [],
      "source": [
        "# Install NLP Profiler for text datasets\n",
        "# !pip install -U -q git+https://github.com/neomatrix369/nlp_profiler@scale-when-applied-to-larger-datasets\n",
        "# print(\"\\n Installation Completed\")\n",
        "\n",
        "!pip install -U -q git+https://github.com/neomatrix369/nlp_profiler.git@master"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4rYvvQ61Ps7"
      },
      "source": [
        "## **Sourcing Data from the Github Respository**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyNa01Ob1PXD"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Create an airline-datasets directory on google colab to host the files\n",
        "!rm -rf airline-datasets\n",
        "!mkdir -p airline-datasets\n",
        "!cd airline-datasets\n",
        "\n",
        "# fetch all the datasets\n",
        "!wget -q --show-progress \"https://raw.githubusercontent.com/billyotieno/analytics-datasets/main/Transport Services/Airlines/african-airlines-reviews-dataset/kenya_airways_flights.csv\" -P ./airline-datasets\n",
        "!wget -q --show-progress \"https://raw.githubusercontent.com/billyotieno/analytics-datasets/main/Transport Services/Airlines/african-airlines-reviews-dataset/air_mauritius.csv\" -P ./airline-datasets\n",
        "!wget -q --show-progress \"https://raw.githubusercontent.com/billyotieno/analytics-datasets/main/Transport Services/Airlines/african-airlines-reviews-dataset/egypt_airways.csv\" -P ./airline-datasets\n",
        "!wget -q --show-progress \"https://raw.githubusercontent.com/billyotieno/analytics-datasets/main/Transport Services/Airlines/african-airlines-reviews-dataset/ethiopian_airlines.csv\" -P ./airline-datasets\n",
        "!wget -q --show-progress \"https://raw.githubusercontent.com/billyotieno/analytics-datasets/main/Transport Services/Airlines/african-airlines-reviews-dataset/fastjet_flights.csv\" -P ./airline-datasets\n",
        "!wget -q --show-progress \"https://raw.githubusercontent.com/billyotieno/analytics-datasets/main/Transport Services/Airlines/african-airlines-reviews-dataset/flysafair_flights.csv\" -P ./airline-datasets\n",
        "!wget -q --show-progress \"https://raw.githubusercontent.com/billyotieno/analytics-datasets/main/Transport Services/Airlines/african-airlines-reviews-dataset/royal_air_maroc.csv\" -P ./airline-datasets\n",
        "!wget -q --show-progress \"https://raw.githubusercontent.com/billyotieno/analytics-datasets/main/Transport Services/Airlines/african-airlines-reviews-dataset/rwand_air_flights.csv\" -P ./airline-datasets\n",
        "!wget -q --show-progress \"https://raw.githubusercontent.com/billyotieno/analytics-datasets/main/Transport Services/Airlines/african-airlines-reviews-dataset/seychelles_airways.csv\" -P ./airline-datasets\n",
        "!wget -q --show-progress \"https://raw.githubusercontent.com/billyotieno/analytics-datasets/main/Transport Services/Airlines/african-airlines-reviews-dataset/south_african_airways.csv\" -P ./airline-datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yuqv8THG2ZXG"
      },
      "source": [
        "## **Importing Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae9LO-162Y-H"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import pandera as pn\n",
        "import dask\n",
        "import seaborn as sns\n",
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import fasttext\n",
        "import warnings\n",
        "import inflect # converting numbers in text to words\n",
        "import wordcloud\n",
        "import missingno as msno\n",
        "from pandas_profiling import ProfileReport\n",
        "from nlp_profiler.core import apply_text_profiling\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from nltk import wordpunct_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from __future__ import print_function\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "# Pandas settings\n",
        "pd.options.mode.chained_assignment = None\n",
        "pd.set_option('display.max_colwidth', 20)\n",
        "pd.options.display.max_rows = 4000\n",
        "from IPython.display import Image\n",
        "\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "# NLTK Download Options\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqXN6Jt-_7uL"
      },
      "outputs": [],
      "source": [
        "# Visualization Fonts\n",
        "!wget -O IBM_Sans.zip \"https://fonts.google.com/download?family=IBM%20Plex%20Sans\"\n",
        "!wget -O McKinsey_Bower.zip \"https://cdn.mckinsey.com/assets/fonts/web/Bower_Fonts.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfGLUR5-hxBn"
      },
      "outputs": [],
      "source": [
        "!unzip -o '*.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQRnqB5jibay"
      },
      "outputs": [],
      "source": [
        "!mv *.ttf /usr/share/fonts/truetype/\n",
        "!mv *.otf /usr/share/fonts/truetype/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sII_fRkJpVcT"
      },
      "source": [
        "## **Loading Data into DataFrames**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQJICKuI1-Ih"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "path = \"./airline-datasets/\"\n",
        "files = Path(path).glob('*.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzGd4yEj8OcO"
      },
      "outputs": [],
      "source": [
        "# Read data into dataframe with a new column identifying airline dataset\n",
        "dfs = list()\n",
        "for f in files:\n",
        "  data = pd.read_csv(f,\n",
        "                     usecols=['Title','Image','Avatar_URL',\n",
        "                              'crvsd','ui_header_link','default',\n",
        "                              'phmbo','phmbo1','dmrsr','dmrsr2','dmrsr3',\n",
        "                              'qwuub_URL','qwuub','tehyy','xcjrc',\n",
        "                              'Rating'])\n",
        "  data['source'] = f.stem\n",
        "  dfs.append(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJQXNtnI-Bwn"
      },
      "outputs": [],
      "source": [
        "df = pd.concat(dfs, ignore_index=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6LOVnaX0mhI"
      },
      "source": [
        "## **Initial Data Exploration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdNUoUxq9XNA"
      },
      "source": [
        "### **Renaming Columns to Clear Columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0GVqGL1QYFE"
      },
      "outputs": [],
      "source": [
        "# Rename dataset to clear & understandable columns\n",
        "column_rename = {\n",
        "    'Title':'review',\n",
        "    'Image':'review_image',\n",
        "    'Avatar_URL':'avatar_url',\n",
        "    'crvsd':'writing_month',\n",
        "    'ui_header_link':'reviewer_username',\n",
        "    'default':'reviewer_city',\n",
        "    'phmbo':'reviewer_contribution',\n",
        "    'phmbo1':'helpful_votes',\n",
        "    'dmrsr':'flight_path',\n",
        "    'dmrsr2':'flight_type',\n",
        "    'dmrsr3':'travel_class',\n",
        "    'qwuub_URL':'review_link',\n",
        "    'qwuub':'review_headline',\n",
        "    'tehyy':'travel_month',\n",
        "    'xcjrc':'disclaimer',\n",
        "    'Rating':'review_rating',\n",
        "    'source':'airline'\n",
        "}\n",
        "\n",
        "df.rename(columns=column_rename, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC55C44hsArk"
      },
      "outputs": [],
      "source": [
        "# Proper Naming for Airlines\n",
        "df['airline'] = df.airline.astype('category')\n",
        "df['airline'] = df['airline'].cat.rename_categories({\n",
        "  'air_mauritius':'Air Mauritius',\n",
        "  'egypt_airways':'Egypt Air',\n",
        "  'ethiopian_airlines':'Ethiopian Airlines',\n",
        "  'fastjet_flights':'FastJet',\n",
        "  'flysafair_flights':'FlySafair',\n",
        "  'kenya_airways_flights':'Kenya Airways',\n",
        "  'royal_air_maroc':'Royal Air Maroc',\n",
        "  'rwand_air_flights':'RwandAir',\n",
        "  'seychelles_airways':'Air Seychelles',\n",
        "  'south_african_airways':'South African Airways',\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkvOC04BZqe6"
      },
      "outputs": [],
      "source": [
        "# Check new columns\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB5iN376RWDo"
      },
      "source": [
        "### **Checking Dataset Shape**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqWpX_6q0GGs"
      },
      "outputs": [],
      "source": [
        "# Checking dataframe shape\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WxPPNl0RcRH"
      },
      "source": [
        "### **Checking DataTypes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rFs-ALC1JI0"
      },
      "outputs": [],
      "source": [
        "# Checking datatypes\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oOLLEXfRfj8"
      },
      "source": [
        "### **Checking for Missing Values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMn56GXvfZym"
      },
      "outputs": [],
      "source": [
        "# Check for Missing Values\n",
        "msno.matrix(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYf9YG2GRjIL"
      },
      "source": [
        "### **Dataset Description**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qJA14sG09Sp"
      },
      "outputs": [],
      "source": [
        "# Checking dataframe description\n",
        "df.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aw93Yk7fo2D3"
      },
      "outputs": [],
      "source": [
        "#Getting the total number of reviews in the dataset\n",
        "n_reviews = df.shape[0]\n",
        "# print('Number of customer reviews in the dataset: {}'.format(n_reviews))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACzVxxEfhimb"
      },
      "outputs": [],
      "source": [
        "review = df[df.airline == 'Kenya Airways'].review.values[12]\n",
        "print(review)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6Peyes6Rowq"
      },
      "source": [
        "### **Initial Data Cleaning: Overlapped Text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzZZm0GZReWP"
      },
      "outputs": [],
      "source": [
        "# Initial dataset cleaning to support exploration\n",
        "def remove_overlapped_text(df):\n",
        "  df = df.copy()\n",
        "  index = df[(df[\"travel_class\"] != \"Economy\") & (df[\"travel_class\"] != \"Business Class\") & (df[\"travel_class\"] != \"First Class\")].index\n",
        "  df.drop(index, inplace=True)\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "  return df\n",
        "\n",
        "df = remove_overlapped_text(df)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO-JbEUMR30e"
      },
      "source": [
        "## **Data Exploration: Focused on Non-Review Columns**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjZz7X0gRw9q"
      },
      "source": [
        "### **Total Number of Reviews by Airlines**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0LVZ1BIUB6e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "viz_color = \"#102747\"\n",
        "\n",
        "# path = '/usr/share/fonts/truetype/IBMPlexSans-Regular.ttf'\n",
        "path = '/usr/share/fonts/truetype/Bower-Bold.otf'\n",
        "fontprop = fm.FontProperties(fname=path)\n",
        "\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "plt.suptitle(\"Number of Reviews by Airline\", ha='left', fontproperties=fontprop, fontsize=40, x=0.125, y=0.98)\n",
        "plt.title(\"Showing distribution of reviews per Airline\", loc='left',alpha=0.9, fontproperties=fontprop, fontsize=20)\n",
        "\n",
        "ax = sns.countplot(data=df, y=\"airline\", ax=ax, color=viz_color, order=df['airline'].value_counts().index)\n",
        "\n",
        "plt.xticks(fontproperties=fontprop, fontsize=15)\n",
        "plt.yticks(fontproperties=fontprop, fontsize=15)\n",
        "plt.xlabel('Airlines', fontproperties=fontprop, fontsize=20)\n",
        "plt.ylabel('Number of Reviews', fontproperties=fontprop, fontsize=20)\n",
        "\n",
        "# ax.set(ylabel=\"Airlines\", xlabel=\"Number of Reviews\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZe6gvL1SO-T"
      },
      "source": [
        "### **Flight Types or Regions Travelled by Reviewers for Each Airline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tmyxj04Dwg_"
      },
      "outputs": [],
      "source": [
        "# What are the most common flight types across the various airlines experience by reviewers?\n",
        "airline_flight_type = df.groupby(['airline', 'flight_type']).size().reset_index().pivot(columns='flight_type', index='airline', values=0)\n",
        "airline_flight_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAxcMAH2TTUS"
      },
      "outputs": [],
      "source": [
        "airline_flight_type.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaFg9VHGWkpU"
      },
      "outputs": [],
      "source": [
        "index= airline_flight_type.index\n",
        "cols = airline_flight_type.columns\n",
        "airline_flight_type.style.background_gradient(cmap='Blues')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVmmWr8LSjsC"
      },
      "source": [
        "### **Distribution of Ratings (1 -5)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WAqUlFeV1Nw"
      },
      "outputs": [],
      "source": [
        "# What is the distribution of Ratings in the review dataset??\n",
        "# Clean up review rating & convert column into ratings / interger\n",
        "df.review_rating = df.review_rating.str[-2:]\n",
        "df.review_rating = df.review_rating.astype(int) / 10\n",
        "df.review_rating.value_counts().plot(kind=\"barh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFC4ts_yTchJ"
      },
      "source": [
        "### **Average Rating Across the Airlines for the Various Travel Classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyC_u2CZH1Or"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "cmap = copy.copy(plt.cm.get_cmap(\"Blues\"))\n",
        "cmap.set_under(\"white\")\n",
        "\n",
        "# Whats the average rating experience by travellers within KQ and across the various airlines in the different classes?\n",
        "import numpy as np\n",
        "\n",
        "plt.figure(figsize=(12, 8), dpi= 80)\n",
        "airline_class_review = df.groupby(['airline', 'travel_class']).agg({'review_rating':[np.mean]}).reset_index().pivot(columns=\"travel_class\", index=\"airline\").droplevel(0, axis=1).droplevel(0, axis=1)\n",
        "airline_class_review.style.background_gradient(cmap='Blues').applymap(lambda x: 'background-color: white' if pd.isna(x) else '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znqKrX7ZTtpb"
      },
      "source": [
        "### **Data Cleaning: Correcting Travel Month Column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc5-UMdgfWMh"
      },
      "outputs": [],
      "source": [
        "# Clean Writing Month\n",
        "df.writing_month = df.writing_month.str[-8:]\n",
        "df.writing_month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDHwdVa_XD7D"
      },
      "outputs": [],
      "source": [
        "# Clean Travel Month\n",
        "df.travel_month = df.travel_month.str[16:]\n",
        "df.travel_month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0JYUGaqZUJH"
      },
      "outputs": [],
      "source": [
        "df['travel_year'] = df.travel_month.str[-4:]\n",
        "df.travel_year.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1spEvbebzh0"
      },
      "outputs": [],
      "source": [
        "# Assumptions, due to extraction error, we'll convert 25** years to 2022\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzY5o-D7T2cU"
      },
      "source": [
        "### **Exploring Review Ratings by Airline and Flight Travel Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7Fbxu80QFZW"
      },
      "outputs": [],
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "plt.suptitle(\"Heatmap of Review Rating by Airline, Travel Class\", ha='left', fontproperties=fontprop, fontsize=30, x=0.125, y=1)\n",
        "plt.title(\"Most airlines tend to have good ratings for their Business Class compared to Economy. \\n\",\n",
        "          loc='left', alpha=0.9, fontproperties=fontprop, fontsize=15)\n",
        "\n",
        "sns.heatmap(airline_class_review, cmap=\"Blues\", linewidth=1, linecolor=\"#F4F4F4\", cbar_kws = {\"location\":\"bottom\", \"use_gridspec\":False})\n",
        "\n",
        "plt.xticks(fontproperties=fontprop, fontsize=15)\n",
        "plt.yticks(fontproperties=fontprop, fontsize=15)\n",
        "plt.xlabel('Flight Travel Class', fontproperties=fontprop, fontsize=20)\n",
        "plt.ylabel('Airlines', fontproperties=fontprop, fontsize=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgNDhVXqtU4x"
      },
      "source": [
        "From the heatmap above, it shows that Travellers have had a great experience with Business Class as opposed to the Economy Class.\n",
        "FlySafari is an exception since it only runs flights in the Economy Class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET3DkYQyt0rf"
      },
      "source": [
        "### **Exploring Review Ratings by Airlines across Regions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9m51NJQLaKU"
      },
      "outputs": [],
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "plt.suptitle(\"Exploring Common Flight Type across Airlines\", ha='left', fontproperties=fontprop, fontsize=30, x=0.125, y=1)\n",
        "plt.title(\"Most of the travel done by airline customers were International followed by Africa. \\n\",\n",
        "          loc='left', alpha=0.9, fontproperties=fontprop, fontsize=15)\n",
        "\n",
        "sns.heatmap(airline_flight_type, cmap=\"Blues\", linewidth=1, linecolor=\"#F4F4F4\")\n",
        "\n",
        "plt.xticks(fontproperties=fontprop, fontsize=15)\n",
        "plt.yticks(fontproperties=fontprop, fontsize=15)\n",
        "plt.xlabel('Flight Type / Regions', fontproperties=fontprop, fontsize=20)\n",
        "plt.ylabel('Airlines', fontproperties=fontprop, fontsize=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ11-Hh1uOZS"
      },
      "source": [
        "From the heatmap above, it shows that most airline travellers took international flights, followed closely by travels to Africa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvNVWzd8uk0Z"
      },
      "source": [
        "### **Breakdown of Airlines by Respective Travel Classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxBwJDGxKQuL"
      },
      "outputs": [],
      "source": [
        "airline_travel_class = df.groupby(['airline', 'travel_class']).size().reset_index().pivot(columns='travel_class', index='airline', values=0)\n",
        "airline_travel_class.reset_index().style.background_gradient(cmap='Blues').applymap(lambda x: 'background-color: white' if pd.isna(x) else '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_QU-cvc94TB"
      },
      "outputs": [],
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "plt.suptitle(\"Travel Classes by Airline\", ha='left', fontproperties=fontprop, fontsize=40, x=0.125, y=0.98)\n",
        "plt.title(\"Most reviewers in the dataset travelled on Economy Class\", loc='left',alpha=0.9, fontproperties=fontprop, fontsize=20)\n",
        "\n",
        "airline_travel_class.plot(kind=\"barh\", stacked=True, ax=ax, color=['darkblue','lightsteelblue','darkred'])\n",
        "\n",
        "plt.xticks(fontproperties=fontprop, fontsize=15)\n",
        "plt.yticks(fontproperties=fontprop, fontsize=15)\n",
        "plt.xlabel('Number of Reviews', fontproperties=fontprop, fontsize=20)\n",
        "plt.ylabel('Airlines', fontproperties=fontprop, fontsize=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SV-pvhr7VHnk"
      },
      "outputs": [],
      "source": [
        "airline_flight_path = df.groupby(['flight_path', 'airline']).size().reset_index().pivot(columns='airline', index='flight_path', values=0)\n",
        "airline_flight_path[\"Kenya Airways\"].sort_values(ascending=False).head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiX4k3fvXyw9"
      },
      "outputs": [],
      "source": [
        "# pd.crosstab(df['flight_path'], df['airline']).plot(kind='barh', stacked=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNpiWbaLUo0l"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "plt.suptitle(\"Top Flight Paths by Reviews - All Airlines\", ha='left', fontproperties=fontprop, fontsize=40, x=0.125, y=0.98)\n",
        "plt.title(\"Flight paths used by customers giving reviews\", loc='left',alpha=0.9, fontproperties=fontprop, fontsize=20)\n",
        "\n",
        "plt.xticks(fontproperties=fontprop, fontsize=12)\n",
        "plt.yticks(fontproperties=fontprop, fontsize=12)\n",
        "\n",
        "df[[\"airline\",'flight_path']].value_counts()[:5].plot(kind='barh', stacked=True, ax=ax, color=['darkblue'])\n",
        "\n",
        "plt.xlabel('Number of Reviews', fontproperties=fontprop, fontsize=20)\n",
        "plt.ylabel('Flight Paths', fontproperties=fontprop, fontsize=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4BctT7YZxnL"
      },
      "outputs": [],
      "source": [
        "df[df.airline == 'Kenya Airways'].flight_path.value_counts()[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p0HjFBpZcvy"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "plt.suptitle(\"Top Flight Paths by Reviews - KQ\", ha='left', fontproperties=fontprop, fontsize=40, x=0.125, y=0.98)\n",
        "plt.title(\"Most flights taken by KQ Reviewers were from London to Nairobi\", loc='left',alpha=0.9, fontproperties=fontprop, fontsize=20)\n",
        "\n",
        "plt.xticks(fontproperties=fontprop, fontsize=12)\n",
        "plt.yticks(fontproperties=fontprop, fontsize=12)\n",
        "\n",
        "df[df.airline == 'Kenya Airways'].flight_path.value_counts()[:15].plot(kind='barh', stacked=True, ax=ax, color=['darkblue'])\n",
        "\n",
        "plt.xlabel('Number of KQ Reviews', fontproperties=fontprop, fontsize=20)\n",
        "plt.ylabel('Flight Paths', fontproperties=fontprop, fontsize=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O-4kPBKiohi"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuUrYY0IRM2a"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Fe-nXgZ3G7"
      },
      "source": [
        "## **Data Exploration: Focused on Review Text**\n",
        "\n",
        "At this step we drill down into the Review Text, Extract text features and perform an exploratory analysis from the extracted features. This feature will then be used downstream in Modelling Stage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agbFtTY-b2Ui"
      },
      "outputs": [],
      "source": [
        "from wordcloud import STOPWORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3dbcN12cavf"
      },
      "outputs": [],
      "source": [
        "type(STOPWORDS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-6A_G6R9UJ6"
      },
      "outputs": [],
      "source": [
        "# Checking on the Word Cloud for Each Review Rating\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "STOPWORDS.update(['flight','airport','airline'])\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "    background_color = 'white',\n",
        "    stopwords = stopwords,\n",
        "    max_words = 400,\n",
        "    max_font_size = 200,\n",
        "    width=1000, height=1000,\n",
        "    random_state = 42\n",
        ").generate(\" \".join(df[\"review\"].astype('str')))\n",
        "\n",
        "fig = plt.figure(figsize = (12,14))\n",
        "plt.imshow(wordcloud)\n",
        "\n",
        "plt.title(\"Frequently Occuring words across all Reviews\", loc='center',alpha=0.9, fontproperties=fontprop, fontsize=22)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0XXPrrq_Gn9"
      },
      "outputs": [],
      "source": [
        "df.review_rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQNk7QUL-0bA"
      },
      "outputs": [],
      "source": [
        "# Checking on the Word Cloud for Each Review Rating\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "STOPWORDS.update(['flight','airport','airline'])\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "    background_color = 'white',\n",
        "    stopwords = stopwords,\n",
        "    max_words = 400,\n",
        "    max_font_size = 200,\n",
        "    width=1000, height=1000,\n",
        "    random_state = 42\n",
        ").generate(\" \".join(df[df.airline == 'Kenya Airways'][\"review\"].astype('str')))\n",
        "\n",
        "fig = plt.figure(figsize = (12,14))\n",
        "plt.imshow(wordcloud)\n",
        "\n",
        "plt.title(\"Frequently Occuring words across all Kenya Airways Reviews\", loc='center',alpha=0.9, fontproperties=fontprop, fontsize=22)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TI1HryZL_pym"
      },
      "outputs": [],
      "source": [
        "# Checking on the Word Cloud for Each Review Rating\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "STOPWORDS.update(['flight','airport','airline'])\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "    background_color = 'white',\n",
        "    stopwords = stopwords,\n",
        "    max_words = 400,\n",
        "    max_font_size = 200,\n",
        "    width=1000, height=1000,\n",
        "    random_state = 42\n",
        ").generate(\" \".join(df[(df.airline == 'Kenya Airways') & (df.review_rating >= 4)][\"review\"].astype('str')))\n",
        "\n",
        "fig = plt.figure(figsize = (12,14))\n",
        "plt.imshow(wordcloud)\n",
        "\n",
        "plt.title(\"Frequent Words - Kenya Airways Reviews (Review Rating >=4) \", loc='center',alpha=0.9, fontproperties=fontprop, fontsize=22)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZnoZqv7AN2Z"
      },
      "outputs": [],
      "source": [
        "# Checking on the Word Cloud for Each Review Rating\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "STOPWORDS.update(['flight','airport','airline'])\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "    background_color = 'white',\n",
        "    stopwords = stopwords,\n",
        "    max_words = 400,\n",
        "    max_font_size = 200,\n",
        "    width=1000, height=1000,\n",
        "    random_state = 42\n",
        ").generate(\" \".join(df[(df.airline == 'Kenya Airways') & (df.review_rating <= 2)][\"review\"].astype('str')))\n",
        "\n",
        "fig = plt.figure(figsize = (12,14))\n",
        "plt.imshow(wordcloud)\n",
        "\n",
        "plt.title(\"Frequent Words - Kenya Airways Reviews (Review Rating <=2) \", loc='center',alpha=0.9, fontproperties=fontprop, fontsize=22)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVaD-s3meFhv"
      },
      "outputs": [],
      "source": [
        "# Checking on the Word Cloud for Each Review Rating\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "STOPWORDS.update(['flight','airport','airline'])\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "    background_color = 'white',\n",
        "    stopwords = stopwords,\n",
        "    max_words = 400,\n",
        "    max_font_size = 200,\n",
        "    width=1000, height=1000,\n",
        "    random_state = 42\n",
        ").generate(\" \".join(df[(df.airline == 'Kenya Airways') & (df.review_rating == 3)][\"review\"].astype('str')))\n",
        "\n",
        "fig = plt.figure(figsize = (12,14))\n",
        "plt.imshow(wordcloud)\n",
        "\n",
        "plt.title(\"Frequent Words - Kenya Airways Reviews (Review Rating ==3) \", loc='center',alpha=0.9, fontproperties=fontprop, fontsize=22)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbLSsSUqAgvk"
      },
      "outputs": [],
      "source": [
        "# From the Dataframe we fetch the Review Column and peform text profilling.\n",
        "text_nlp = pd.DataFrame(df, columns=['review'])\n",
        "# Exploring a Sample Review\n",
        "text_nlp[\"review\"][2000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as-nODpL52OC"
      },
      "outputs": [],
      "source": [
        "# Text NLP Review for Kenya Airways Data\n",
        "text_nlp_kq = pd.DataFrame(df[df.airline == 'Kenya Airways'], columns=['review'])\n",
        "\n",
        "profile_data_kq = apply_text_profiling(\n",
        "    text_nlp_kq, 'review',\n",
        "    params={'spelling_check': False,\n",
        "            'grammar_check': False,\n",
        "            'ease_of_reading_check':False,\n",
        "            'parallelisation_method': 'default'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98C2QMR36yeP"
      },
      "outputs": [],
      "source": [
        "profile_data_kq.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdE0m7Xr7joa"
      },
      "outputs": [],
      "source": [
        "profile_data_kq[[\"sentences_count\",\n",
        "\"characters_count\",\n",
        "\"repeated_letters_count\",\n",
        "\"spaces_count\",\n",
        "\"chars_excl_spaces_count\",\n",
        "\"repeated_spaces_count\",\n",
        "\"whitespaces_count\",\n",
        "\"chars_excl_whitespaces_count\",\n",
        "\"repeated_whitespaces_count\",\n",
        "\"count_words\",\n",
        "\"duplicates_count\",\n",
        "\"emoji_count\",\n",
        "'repeated_digits_count',\n",
        "\"whole_numbers_count\",\n",
        "\"alpha_numeric_count\",\n",
        "\"non_alpha_numeric_count\",\n",
        "\"punctuations_count\",\n",
        "\"repeated_punctuations_count\",\n",
        "\"stop_words_count\",\n",
        "\"dates_count\",\n",
        "\"noun_phrase_count\",\n",
        "\"english_characters_count\",\n",
        "\"non_english_characters_count\",\n",
        "\"syllables_count\"]].aggregate('sum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvahDmB4_V1V"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style=\"ticks\")\n",
        "\n",
        "pairplot_df = profile_data_kq[['sentiment_polarity_summarised','stop_words_count','emoji_count','sentences_count','punctuations_count']]\n",
        "sns.pairplot(pairplot_df, hue='sentiment_polarity_summarised')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laXFWtWsBYaY"
      },
      "outputs": [],
      "source": [
        "pairplot_df = profile_data_kq[['sentiment_polarity_summarised','stop_words_count','emoji_count','sentences_count','punctuations_count']]\n",
        "sns.pairplot(pairplot_df, hue='sentiment_polarity_summarised')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZetnG3AAoHf"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "# We'll skip this step due to the execution timeline on the notebook.\n",
        "# As an alternative we have saved the files from this output into .csv so that\n",
        "# we only read directly from the CSV files.\n",
        "profile_data_kq = apply_text_profiling(\n",
        "    text_nlp, 'review',\n",
        "    params={'spelling_check': False,\n",
        "            'grammar_check': False,\n",
        "            'ease_of_reading_check':False,\n",
        "            'parallelisation_method': 'default'})\n",
        "\n",
        "# Generating a profiling report into HTML\n",
        "profile_text = ProfileReport(profile_data)\n",
        "profile_text.to_file(\"airline-review-text-profiler-form.html\")\n",
        "\n",
        "# Saving the profiled data to CSV to save on execution\n",
        "profile_data.to_csv(\"airline-review-text-profiled-dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuDlBOl4IZsw"
      },
      "outputs": [],
      "source": [
        "profile_data = pd.read_csv(\"https://raw.githubusercontent.com/billyotieno/analytics-datasets/main/Transport%20Services/Airlines/airline-review-text-profiled-dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyINU8UjawfV"
      },
      "outputs": [],
      "source": [
        "# Dropping the Unnamed: 0 column created during file export\n",
        "profile_data.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
        "profile_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6nPP1hcbNDK"
      },
      "outputs": [],
      "source": [
        "# Check the datatypes of the newly created\n",
        "profile_data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkxGFNpEw0QI"
      },
      "outputs": [],
      "source": [
        "profile_data.iloc[9500,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wX2b1P2wj1N"
      },
      "outputs": [],
      "source": [
        "profile_data.iloc[9500,]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEjNF3QrKksz"
      },
      "outputs": [],
      "source": [
        "# Comparing Common Words used by Different Airlines - we'll use df for this\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzB1YP-Skvof"
      },
      "outputs": [],
      "source": [
        "# Summary of key text statistics\n",
        "print(\"Number of Emojis in Corpus - \", profile_data[\"emoji_count\"].sum())\n",
        "print(\"Number of Punctuations in Corpus - \", profile_data[\"punctuations_count\"].sum())\n",
        "print(\"Number of Stop Words in Corpus - \", profile_data[\"stop_words_count\"].sum())\n",
        "print(\"Number of Dates in Corpus - \", profile_data[\"dates_count\"].sum())\n",
        "print(\"Number of Non-English Character in Corpus - \", profile_data[\"non_english_characters_count\"].sum())\n",
        "print(\"Number of Repeated Whitespaces in Corpus - \", profile_data[\"repeated_whitespaces_count\"].sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KTia-Ad8L6w"
      },
      "source": [
        "### **Checking for NaNs in Extracted Review Columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQXTk8YG8SJF"
      },
      "outputs": [],
      "source": [
        "# Percentage of non-null values.\n",
        "filling_rates = 100.*profile_data.count().sort_values(ascending=False)/profile_data.shape[0]\n",
        "print(filling_rates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmiNnrqMIndF"
      },
      "outputs": [],
      "source": [
        "values_filling_rates = filling_rates.values\n",
        "text_filling_rates = filling_rates.index.to_list()\n",
        "print(text_filling_rates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9mrf46O_cB0"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,6),dpi=100)\n",
        "sns.set(style=\"whitegrid\")\n",
        "ax = sns.barplot(x=values_filling_rates, y=text_filling_rates,color=\"Red\")\n",
        "ax.set(xlabel='Filling percentage (%)', ylabel='Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvyCIsjiAseg"
      },
      "source": [
        "### **Features distributions into Boolean, Categorical and Numerical types**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kakIf_XPBEV1"
      },
      "outputs": [],
      "source": [
        "df_for_training = profile_data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvcsTEICA9JC"
      },
      "outputs": [],
      "source": [
        "cols_for_training = df_for_training.columns.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APainH04AGyD"
      },
      "outputs": [],
      "source": [
        "feats_bool = ['recommended',\n",
        "              'has_layover']\n",
        "feats_cat = ['airline',\n",
        "             'traveller_type',\n",
        "             'cabin','review_text', 'review',\n",
        "             'pos_neu_neg_review_score']\n",
        "feats_num = [feat for feat in cols_for_training if feat not in feats_bool and feat not in feats_cat]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AULQY-jYBhA3"
      },
      "outputs": [],
      "source": [
        "print('Boolean features: \\n{}\\n'.format(feats_bool))\n",
        "print('Categorical features: \\n{}\\n'.format(feats_cat))\n",
        "print('Numerical features: \\n{}\\n'.format(feats_num))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V63NJ4jBlXl"
      },
      "source": [
        "### **Plotting the correlation matrix for the features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKz0ZD1aBrCS"
      },
      "outputs": [],
      "source": [
        "# Let's plot a correlation matrix among the features\n",
        "def plot_cmap(matrix_values, figsize_w, figsize_h, filename):\n",
        "    \"\"\"\n",
        "    Plot a heatmap corresponding to the input values.\n",
        "    \"\"\"\n",
        "    if figsize_w is not None and figsize_h is not None:\n",
        "        plt.figure(figsize=(figsize_w,figsize_h))\n",
        "    else:\n",
        "        plt.figure()\n",
        "    cmap = sns.diverging_palette(240, 10, sep=20, as_cmap=True)\n",
        "    sns.heatmap(matrix_values, fmt=\".2f\", cmap=cmap, vmin=-1, vmax=1)\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "    return cmap\n",
        "\n",
        "corr_values = df_for_training[feats_num].dropna(axis=0,how='any').corr()\n",
        "plot_cmap(matrix_values=corr_values,\n",
        "          figsize_w=15,\n",
        "          figsize_h=15,\n",
        "          filename='./Corr.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqKqzSwxCA28"
      },
      "source": [
        "Note:\n",
        "\n",
        "1. A positive correlation between the different types of review scores and subscores\n",
        "2. A negative correlation between the length of the review text and the value of the different types of review scores and subscores\n",
        "3. The similarity between using the number of characters and the number of words, from which we conclude that we can drop one of the two features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsdMDQLTvJij"
      },
      "outputs": [],
      "source": [
        "# Based on the Correlation Matrix - Checking the Columnns to be Dropped\n",
        "corr_matrix = profile_data.corr().abs()\n",
        "\n",
        "# Select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "\n",
        "# Find features with correlation greater than 0.95\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.80)]\n",
        "\n",
        "to_drop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZosZNZ4mAvp"
      },
      "source": [
        "## **Data Quality Summary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91trJ6e0mKlE"
      },
      "source": [
        "As an output of our Data Exploration efforts we have been able to identify the following Data Quality Issues within the Review Text and additional columns from the datasets. Below is a summary of our Data Quality findings which informs our Data Preparation Stage:\n",
        "\n",
        "**Data Quality Issues Identified in Non-Review Text Columns**\n",
        "\n",
        "> - *Redundant Columns*   \n",
        "> - *Duplicate Rows*  \n",
        "> - *Wrong Data Types*\n",
        "> - *Missing Values*\n",
        "\n",
        "**Data Quality Issues Identified in Review Text**\n",
        "\n",
        "> - *Extra Whitespaces in Text*.\n",
        "> - *Digitis in Review Text*.  \n",
        "> - *Existing Emoji's in Review Text*.  \n",
        "> - *Punctuations*.  \n",
        "> - *URL's*.  \n",
        "\n",
        "For the review text, the data will be cleaned up at part of text pre-processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqaDRPCsq64H"
      },
      "source": [
        "## **Data Preparation**\n",
        "\n",
        "At this stage we prepare the data for modelling and further analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDXZ9KbArND8"
      },
      "source": [
        "### **1. Merging the two Datasets - Text Profiled & Non-Review Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xId35H1JrZ4i"
      },
      "outputs": [],
      "source": [
        "# profile_data\n",
        "profile_data[\"id\"] = profile_data.index\n",
        "df[\"id\"] = df.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W381yrSHwJRd"
      },
      "outputs": [],
      "source": [
        "review_df = pd.merge(df, profile_data, how=\"left\", on=\"id\")\n",
        "review_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIYTrlFt9OxX"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFjFtqjZ9Q7S"
      },
      "outputs": [],
      "source": [
        "profile_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2rzno12rhNA"
      },
      "outputs": [],
      "source": [
        "review_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRu-gfMtrMO1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a07rqK3amAff"
      },
      "outputs": [],
      "source": [
        "review_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJtzElqxuSOG"
      },
      "outputs": [],
      "source": [
        "review_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOz3zjCv_bHY"
      },
      "outputs": [],
      "source": [
        "review_df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clfUWwL0Yxqm"
      },
      "source": [
        "### **2. Removing Duplicate Rows**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xinMLBZnYxQz"
      },
      "outputs": [],
      "source": [
        "# Number of Duplicate Rows\n",
        "print(review_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYsbet3FXjwg"
      },
      "source": [
        "### **3. Removing Redundant / Unrequired Columns** - Select Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nryLa4mG_imz"
      },
      "outputs": [],
      "source": [
        "redundant_columns = ['spaces_count',\n",
        " 'chars_excl_spaces_count',\n",
        " 'whitespaces_count',\n",
        " 'chars_excl_whitespaces_count',\n",
        " 'count_words',\n",
        " 'duplicates_count',\n",
        " 'alpha_numeric_count',\n",
        " 'non_alpha_numeric_count',\n",
        " 'stop_words_count',\n",
        " 'noun_phrase_count',\n",
        " 'english_characters_count',\n",
        " 'syllables_count',\n",
        " 'review_y',\n",
        " 'review_image',\n",
        " 'avatar_url',\n",
        " 'reviewer_username',\n",
        " 'reviewer_city',\n",
        " 'helpful_votes',\n",
        " 'review_link',\n",
        " 'review_headline',\n",
        " 'id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmvAP3dpaLh-"
      },
      "outputs": [],
      "source": [
        "review_df.drop(redundant_columns, axis=1, inplace=True)\n",
        "review_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn8xHRVEcdwc"
      },
      "outputs": [],
      "source": [
        "review_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDt1YXyDc74S"
      },
      "source": [
        "### **4. Cleaning Travel Month & Year** - Select Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pv3JOh9pkXUd"
      },
      "outputs": [],
      "source": [
        "# Replace month abbreviation in writing month\n",
        "def replace_month_abrev(date_string):\n",
        "    month_dict = {\"Jan \": \"January \",\n",
        "              \"Feb \": \"February \",\n",
        "              \"Mar \": \"March \",\n",
        "              \"Apr \": \"April \",\n",
        "              \"May \": \"May \",\n",
        "              \"Jun \": \"June \",\n",
        "              \"Jul \": \"July \",\n",
        "              \"Aug \": \"August \",\n",
        "              \"Sep \": \"September \",\n",
        "              \"Sept \": \"September \",\n",
        "              \"Oct \": \"October \",\n",
        "              \"Nov \": \"November \",\n",
        "              \"Dec \": \"December \"}\n",
        "    # find all dates with abrev\n",
        "    abrev_found = filter(lambda abrev_month: abrev_month in date_string, month_dict.keys())\n",
        "    # replace each date with its abbreviation\n",
        "    for abrev in abrev_found:\n",
        "        date_string = date_string.replace(abrev, month_dict[abrev])\n",
        "    # return the modified string (or original if no states were found)\n",
        "    return date_string\n",
        "\n",
        "review_df.writing_month = review_df.writing_month.apply(replace_month_abrev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Wf2mhSXce1m"
      },
      "outputs": [],
      "source": [
        "# Removing all the with wrong travel periods and replacing them with null\n",
        "review_df[review_df.travel_year >= '2500'].travel_month = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OnQ_XxFl_gn"
      },
      "outputs": [],
      "source": [
        "review_df.loc[review_df.travel_year >= '2500','travel_month'] = review_df.loc[review_df.travel_year >= '2500','writing_month']\n",
        "review_df.loc[review_df.travel_year >= '2500','travel_month']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_lv5mwAofrj"
      },
      "outputs": [],
      "source": [
        "review_df.loc[review_df.travel_year >= '2500', 'travel_year'] = review_df.travel_month.str[-4:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xi4K8pl6jqVl"
      },
      "outputs": [],
      "source": [
        "# Where travel month is missing, we assume the date review was written is the same as the travel month\n",
        "review_df.travel_month.fillna(review_df.writing_month, inplace=True)\n",
        "review_df.travel_year.fillna(review_df.writing_month.str[-4:], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d29TagCIm3AS"
      },
      "outputs": [],
      "source": [
        "review_df[review_df.travel_month.isna()].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo6QJuKxdQku"
      },
      "outputs": [],
      "source": [
        "review_df.travel_year.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi2HJQwnoZHG"
      },
      "outputs": [],
      "source": [
        "# Check again for missing values - No missing values in dataset - Travel is Cleaned.\n",
        "review_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwqy97PLHylu"
      },
      "outputs": [],
      "source": [
        "review_df.travel_month.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vGqMvpppx4u"
      },
      "outputs": [],
      "source": [
        "# Create new column travel date\n",
        "review_df[\"travel_date\"] = pd.to_datetime(review_df['travel_month'], format=\"%B %Y\")\n",
        "review_df.travel_date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpXMSwBvsqE0"
      },
      "source": [
        "### **5. DataType Conversion**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thsYISP7s34J"
      },
      "source": [
        "At this stage we convert the *reviewer contributions* from string to integer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFwVQZ65sj2E"
      },
      "outputs": [],
      "source": [
        "review_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ukIZRyStVrt"
      },
      "outputs": [],
      "source": [
        "review_df.reviewer_contribution = \\\n",
        "review_df.reviewer_contribution.str.replace('contributions','').str.replace('contribution','').str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGgB-HiS7UBa"
      },
      "outputs": [],
      "source": [
        "review_df.reviewer_contribution = review_df.reviewer_contribution.astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CBTTd-Da8RB"
      },
      "source": [
        "### **6. Review Sentiment - New Column from Rating Scores**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPrKuACMbHI4"
      },
      "source": [
        "All Ratings >= 4 are classified as \"Positive Reviews - 1\".    \n",
        "All Ratings < 4 are classified as \"Negative Reviews - 0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-WUQjhga7xQ"
      },
      "outputs": [],
      "source": [
        "def convert_rating_to_sentiment(rating):\n",
        "  return 1 if rating >=4 else 0\n",
        "\n",
        "review_df[\"review_sentiment\"] = review_df.review_rating.apply(convert_rating_to_sentiment)\n",
        "review_df.review_sentiment.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYDTSIYKKo_D"
      },
      "outputs": [],
      "source": [
        "review_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IGHWEtRH0dV"
      },
      "outputs": [],
      "source": [
        "review_df = review_df[review_df.airline == 'Kenya Airways']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8DIrwWyCFaj"
      },
      "source": [
        "## **Data Preparation - Text Pre-processing for Reviews**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQUuWmkdC866"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyAJoXJjETj2"
      },
      "outputs": [],
      "source": [
        "# Pre-processing Variables Declared\n",
        "CONTRACTION_MAP = {\"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\",\n",
        "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
        "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\n",
        "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\",\n",
        "                   \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
        "                   \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\",\n",
        "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",\n",
        "                   \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\",\n",
        "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
        "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
        "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
        "                   \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\",\n",
        "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
        "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\",\n",
        "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
        "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n",
        "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
        "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
        "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\",\n",
        "                   \"this's\": \"this is\",\n",
        "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
        "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
        "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
        "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
        "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
        "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
        "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
        "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
        "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
        "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n",
        "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
        "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
        "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                   \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n",
        "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
        "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", \"n't\": \"not\"}\n",
        "\n",
        "PUNCTUATIONS = [\n",
        "    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
        "    '/', '[', ']', '>', '%', '=', '#', '*', '+', \"\\\\\", \"*\",  \"~\", \"@\", \"£\",\n",
        "    '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n",
        "    '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n",
        "    '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n",
        "    '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n",
        "    '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
        "    'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n",
        "    '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n",
        "    '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤','*']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzZje1Kk1QV-"
      },
      "outputs": [],
      "source": [
        "!pip install pyspellchecker\n",
        "!pip install textdistance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CarsWcB_CGe8"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Text Pre-processing Function\n",
        "def expand_contractions(sentence, contraction_mapping):\n",
        "\n",
        "    \"\"\"Function expands a contraction word within a sentence\n",
        "       returns a sentence with expanded contraction. example; can't to cannot\n",
        "\n",
        "    Args:\n",
        "        sentence (str): A sentence with a contraction word\n",
        "        contraction_mapping (dict): A list of contraction and their expanded forms.\n",
        "\n",
        "    Returns:\n",
        "        str: A string with expanded contractions\n",
        "    \"\"\"\n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(\n",
        "        contraction_mapping.keys())),\n",
        "        flags=re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(\n",
        "            match) else contraction_mapping.get(match.lower())\n",
        "        expanded_contraction = first_char + expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "\n",
        "    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
        "    return expanded_sentence\n",
        "\n",
        "\n",
        "def remove_emojis(text):\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U00002700-\\U000027BF\"  # Dingbats\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
        "        u\"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # Miscellaneous Symbols And Pictographs\n",
        "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
        "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoj, '', text)\n",
        "\n",
        "\n",
        "def sep_num_words(text):\n",
        "    \"\"\"\n",
        "    Function seperates numbers from words or other characters e.g. 22ABC to 22 ABC\n",
        "    Args:\n",
        "        text (str): String of\n",
        "    Returns:\n",
        "        cleaned string with numbers seperated from words and other characters\n",
        "    \"\"\"\n",
        "    return re.sub(r\"([0-9]+(\\.[0-9]+)?)\", r\"\\1 \", text).strip()\n",
        "\n",
        "\n",
        "def num_to_word(text):\n",
        "    \"\"\"Function converts numbers in review texts into words.\n",
        "\n",
        "    Args:\n",
        "        text (str): A review text\n",
        "\n",
        "    Returns:\n",
        "        str: A review text with numbers converted to words.\n",
        "    \"\"\"\n",
        "    p = inflect.engine()\n",
        "    output_text = []\n",
        "    for word in text.split():\n",
        "        if word.isdigit():\n",
        "          output_text.append(p.number_to_words(word))\n",
        "        else:\n",
        "          output_text.append(word)\n",
        "    return \" \".join(output_text)\n",
        "\n",
        "def remove_numbers_in_string(text):\n",
        "     mapping = str.maketrans('', '', string.digits)\n",
        "     text = text.translate(mapping)\n",
        "     return text\n",
        "\n",
        "def classify_reviews_by_lang(df, review_column=\"review_x\"):\n",
        "    \"\"\"Function classifies reviews by language and adds a new column to the dataframe.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): Reviews DataFrame\n",
        "\n",
        "    Returns:\n",
        "        df: A Reviews Dataframe with a new column with classifications.\n",
        "    \"\"\"\n",
        "    # Loading pre-trained language model to identify the review languages - ## The objective is to only focus on english review.\n",
        "    pre_trained_model = \"lid.176.bin\"\n",
        "    lang_model = fasttext.load_model(pre_trained_model)\n",
        "\n",
        "    # For each review line, I pass it through the model .predict() function with a resulting language - en,de etc.\n",
        "    detected_lang  = []\n",
        "\n",
        "    for review in df[review_column]:\n",
        "        language = lang_model.predict(review)[0]\n",
        "        detected_lang.append(str(language)[11:13])\n",
        "\n",
        "    df[\"review_language\"] = detected_lang\n",
        "    return df\n",
        "\n",
        "\n",
        "def drop_non_english_languages(df):\n",
        "    \"\"\"Function drops all the non-english languages from the DataFrame\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): DataFrame Object\n",
        "\n",
        "    Returns:\n",
        "        df: Returns a DataFrame with non-english languages dropped.\n",
        "    \"\"\"\n",
        "    other_language_index = df[(df[\"review_language\"] != 'en')].index\n",
        "    df = df.drop(other_language_index)\n",
        "    return df\n",
        "\n",
        "\n",
        "def remove_punctuation(text, punctuations):\n",
        "    \"\"\"Function removes all punctuations from text.\n",
        "\n",
        "    Args:\n",
        "        text (str): A string text with or without punctuations.\n",
        "        punctuations (List): A list of common punctuations.\n",
        "\n",
        "    Returns:\n",
        "        str: Returns a string cleaned of punctuations\n",
        "    \"\"\"\n",
        "    for punctuation in punctuations:\n",
        "        if punctuation in text:\n",
        "            text = text.replace(punctuation, '')\n",
        "        return text.strip().lower()\n",
        "\n",
        "def remove_small_character(token_list, threshold=2):\n",
        "    return [word for word in token_list if len(word) > threshold]\n",
        "\n",
        "\n",
        "def remove_punctuation_list(word_token, punctuations):\n",
        "    \"\"\"Function removes all punctuations from a List of Tokens.\n",
        "\n",
        "    Args:\n",
        "        word_token (List): A list of Word Tokens.\n",
        "        punctuations (List): A list of punctuations.\n",
        "\n",
        "    Returns:\n",
        "        List: A list of word tokens without punctuation tokens.\n",
        "    \"\"\"\n",
        "    for word in word_token:\n",
        "        if word in punctuations:\n",
        "          word_token.remove(word)\n",
        "    return word_token\n",
        "\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    \"\"\"Function removes stop words from word token list.\n",
        "\n",
        "    Args:\n",
        "        text (str): Review text corpus.\n",
        "\n",
        "    Returns:\n",
        "        List: A list of word token without stop words.\n",
        "    \"\"\"\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "    return [t for t in text if not t in stopwords_set]\n",
        "\n",
        "\n",
        "def lemmatize_review(token_list):\n",
        "    \"\"\"\n",
        "    Function takes the lemmatizer object and word_token_list\n",
        "    Input: Lemmatizer, word_token_list []\n",
        "    Output: A list of lemmatized word tokens\n",
        "    Return\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(w) for w in token_list]\n",
        "\n",
        "def remove_punctuations_in_token(token_strings):\n",
        "  \"\"\" Function removes punctuations from a token list.\n",
        "  \"\"\"\n",
        "  return [x for x in token_strings if not re.fullmatch('[' + string.punctuation + ']+', x)]\n",
        "\n",
        "def preprocess_airline_reviews(df, CONTRACTION_MAP, PUNCTUATIONS):\n",
        "    \"\"\"\n",
        "    Function take Dataframe, Contraction Mapping and Punctuations and returns a cleaned DataFrame\n",
        "    with Tokenized Review Column\n",
        "    Args:\n",
        "        df (DataFrame): A DataFrame Object\n",
        "        CONTRACTION_MAP (dict): A dictionary of contractions\n",
        "        PUNCTUATIONS (list): A list of punctuations to remove from text\n",
        "    Returns:\n",
        "        Returns a DataFrame Object\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    # create a new review column for initial review column\n",
        "    df[\"processed_review_tokens\"] = df[\"review_x\"]\n",
        "    df[\"processed_review_tokens\"] = df[\"processed_review_tokens\"].apply(lambda x: expand_contractions(x, CONTRACTION_MAP))\n",
        "    df[\"processed_review_tokens\"] = df[\"processed_review_tokens\"].apply(lambda x: sep_num_words(x))\n",
        "    df[\"processed_review_tokens\"] = df[\"processed_review_tokens\"].apply(lambda x: remove_numbers_in_string(x))\n",
        "    df[\"processed_review_tokens\"] = df[\"processed_review_tokens\"].apply(lambda x: num_to_word(x))\n",
        "    df[\"processed_review_tokens\"] = df[\"processed_review_tokens\"].apply(lambda x: remove_emojis(x))\n",
        "\n",
        "    # Classify reviews by language\n",
        "    # classify_reviews_by_lang(df, \"review_x\")\n",
        "    # Drop non-english reviews\n",
        "    # drop_non_english_languages(df)\n",
        "\n",
        "    df[\"processed_review_tokens\"] = df[\"processed_review_tokens\"].apply(lambda x: remove_punctuation(x, PUNCTUATIONS))\n",
        "    df[\"processed_review_tokens\"] = df[\"processed_review_tokens\"].apply(lambda x: wordpunct_tokenize(x))\n",
        "    df[\"processed_review_tokens\"] = df[\"processed_review_tokens\"].apply(lambda x: remove_punctuation_list(x, PUNCTUATIONS))\n",
        "    df[\"processed_review_tokens\"] = df[\"processed_review_tokens\"].apply(lambda x: remove_punctuations_in_token(x))\n",
        "    df[\"processed_review_tokens\"] = df[\"processed_review_tokens\"].apply(lambda x: remove_small_character(x, threshold=2))\n",
        "    df[\"processed_review_tokens\"] = df[\"processed_review_tokens\"].apply(lambda x: remove_stop_words(x))\n",
        "    df[\"processed_review_tokens\"] = df[\"processed_review_tokens\"].apply(lambda x: lemmatize_review(x))\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7ZJfv1TECWO"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "review_df = preprocess_airline_reviews(\n",
        "    review_df,\n",
        "    CONTRACTION_MAP,\n",
        "    PUNCTUATIONS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMIGa4E9Ez3z"
      },
      "outputs": [],
      "source": [
        "review_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9h8lzbYLHr-"
      },
      "outputs": [],
      "source": [
        "review_df.iloc[200,-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIFr98omeGvK"
      },
      "source": [
        "## **Exploratory Data Analysis / Modelling**\n",
        "### Building a Quick Sentiment Classifier using CountVectorizer on Airline Reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_r97WQILKer"
      },
      "outputs": [],
      "source": [
        "review_df.sentiment_polarity_summarised.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5QwiseeO_gN"
      },
      "outputs": [],
      "source": [
        "review_df.sentiment_subjectivity.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PovpmDSeMN_e"
      },
      "outputs": [],
      "source": [
        "review_df.review_sentiment.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bCd0tH-MZJY"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(12, 12))\n",
        "sns.boxplot(x=\"review_rating\", y=\"sentiment_polarity_score\", hue='travel_class',\n",
        "                data=review_df, ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L6rIDbSOrBC"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(12, 12))\n",
        "sns.boxplot(x=\"review_rating\", y=\"sentiment_subjectivity_score\", hue='travel_class',\n",
        "                data=review_df, ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTCLOdytNwuu"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(12, 12))\n",
        "sns.boxplot(x=\"review_rating\", y=\"sentiment_polarity_score\", hue='flight_type',\n",
        "                data=review_df, ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbeC5AeVOXaw"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(12, 12))\n",
        "sns.boxplot(x=\"review_rating\", y=\"sentiment_subjectivity_score\", hue='flight_type',\n",
        "                data=review_df, ax=ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWFPpzyCybxa"
      },
      "outputs": [],
      "source": [
        "### **Text Detokenization - After pre-processing**\n",
        "\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "def token_to_sentence(token):\n",
        "    return TreebankWordDetokenizer().detokenize(token)\n",
        "\n",
        "review_df['processed_review_detokenized'] = review_df.processed_review_tokens.apply(lambda x: token_to_sentence(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7nriY8wypJq"
      },
      "outputs": [],
      "source": [
        "review_df[\"processed_review_detokenized\"].values[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYWaJogIP2KH"
      },
      "outputs": [],
      "source": [
        "review_df.iloc[10,:].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sqio6lqT3894"
      },
      "outputs": [],
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell_corrector = SpellChecker()\n",
        "\n",
        "# spelling correction using spellchecker\n",
        "def spell_correction(text):\n",
        "\t\"\"\"\n",
        "\tReturn :- text which have correct spelling words\n",
        "\tInput :- string\n",
        "\tOutput :- string\n",
        "\t\"\"\"\n",
        "\t# initialize empty list to save correct spell words\n",
        "\tcorrect_words = []\n",
        "\t# extract spelling incorrect words by using unknown function of spellchecker\n",
        "\tmisSpelled_words = spell_corrector.unknown(text.split())\n",
        "\n",
        "\tfor each_word in text.split():\n",
        "\t\tif each_word in misSpelled_words:\n",
        "\t\t\tright_word = spell_corrector.correction(each_word)\n",
        "\t\t\tcorrect_words.append(right_word)\n",
        "\t\telse:\n",
        "\t\t\tcorrect_words.append(each_word)\n",
        "\n",
        "\t# joining correct_words list into single string\n",
        "\tcorrect_spelling = ' '.join(word for word in correct_words if word)\n",
        "\treturn correct_spelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXAhFmnL3_a7"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "review_df[\"processed_review_detokenized\"] = review_df[\"processed_review_detokenized\"].apply(lambda x: spell_correction(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9Ow5ul0ZdO9"
      },
      "source": [
        "### POS - Review Text Parts of Speech Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBxUhVUVyYgC"
      },
      "outputs": [],
      "source": [
        "### **Review Text - Parts of Speech Analysis**\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def pos_tag(text):\n",
        "    df = pd.DataFrame(columns = ['WORD', 'POS'])\n",
        "    doc = nlp(text)\n",
        "    for token in doc:\n",
        "        df = df.append({'WORD': token.text, 'POS': token.pos_}, ignore_index=True)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdSymcwwy81h"
      },
      "outputs": [],
      "source": [
        "df_pos = pos_tag(review_df['processed_review_detokenized'].to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klfYMH4YzPfy"
      },
      "outputs": [],
      "source": [
        "df_pos.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7W69X29zYwO"
      },
      "outputs": [],
      "source": [
        "df_top_pos = df_pos.groupby('POS')['POS'].count().\\\n",
        "    reset_index(name='count').sort_values(['count'],ascending=False).head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-ksgVBBzf6T"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "plt.suptitle(\"Review Text POS Analysis - KQ\", ha='left', fontproperties=fontprop, fontsize=40, x=0.125, y=0.98)\n",
        "plt.title(\"Review Text is composed mainly of Spaces, then Nouns\", loc='left',alpha=0.9, fontproperties=fontprop, fontsize=20)\n",
        "\n",
        "plt.xticks(fontproperties=fontprop, fontsize=12)\n",
        "plt.yticks(fontproperties=fontprop, fontsize=12)\n",
        "\n",
        "df_top_pos.plot(kind=\"bar\", x='POS', stacked=True, ax=ax, color=['darkblue'])\n",
        "\n",
        "plt.xlabel('Parts of Speech Tags', fontproperties=fontprop, fontsize=20)\n",
        "plt.ylabel('Frequency in Review Text', fontproperties=fontprop, fontsize=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BoyHFfVzyAF"
      },
      "outputs": [],
      "source": [
        "df_nn = df_pos[df_pos['POS'] == 'NOUN'].copy()\n",
        "df_nn.groupby('WORD')['WORD'].count().reset_index(name='count').\\\n",
        "    sort_values(['count'], ascending=False).head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrbFQBbHWRL8"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "plt.suptitle(\"POS Analysis (Nouns) - KQ\", ha='left', fontproperties=fontprop, fontsize=40, x=0.125, y=0.98)\n",
        "plt.title(\"Top ranking noun in frequency is Flight, Time\", loc='left',alpha=0.9, fontproperties=fontprop, fontsize=20)\n",
        "\n",
        "plt.xticks(fontproperties=fontprop, fontsize=12)\n",
        "plt.yticks(fontproperties=fontprop, fontsize=12)\n",
        "\n",
        "df_nn = df_pos[df_pos['POS'] == 'NOUN'].copy()\n",
        "df_nn.groupby('WORD')['WORD'].count().reset_index(name='count').\\\n",
        "    sort_values(['count'], ascending=False).head(15).plot(kind='barh', x='WORD', stacked=True, ax=ax, color=['darkblue'])\n",
        "\n",
        "plt.xlabel('Frequency of Words', fontproperties=fontprop, fontsize=20)\n",
        "plt.ylabel('Nouns in Review', fontproperties=fontprop, fontsize=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22d5HrQ1UA-s"
      },
      "outputs": [],
      "source": [
        "# df_nn = df_pos[df_pos['POS'] == 'VERB'].copy()\n",
        "# df_nn.groupby('WORD')['WORD'].count().reset_index(name='count').\\\n",
        "#     sort_values(['count'], ascending=False).head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ruyGU5GXc8j"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "\n",
        "plt.suptitle(\"POS Analysis (Adjective) - KQ\", ha='left', fontproperties=fontprop, fontsize=40, x=0.125, y=0.98)\n",
        "plt.title(\"Top ranking adjective in frequency is Flight, Time\", loc='left',alpha=0.9, fontproperties=fontprop, fontsize=20)\n",
        "\n",
        "plt.xticks(fontproperties=fontprop, fontsize=12)\n",
        "plt.yticks(fontproperties=fontprop, fontsize=12)\n",
        "\n",
        "df_nn = df_pos[df_pos['POS'] == 'ADJ'].copy()\n",
        "df_nn.groupby('WORD')['WORD'].count().reset_index(name='count').\\\n",
        "    sort_values(['count'], ascending=False).head(15).plot(kind='barh', x='WORD', stacked=True, ax=ax, color=['darkblue'])\n",
        "\n",
        "plt.xlabel('Frequency of Words', fontproperties=fontprop, fontsize=20)\n",
        "plt.ylabel('Nouns in Review', fontproperties=fontprop, fontsize=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuLIRERmz4Hh"
      },
      "outputs": [],
      "source": [
        "df_adj = df_pos[df_pos['POS'] == 'ADJ'].copy()\n",
        "df_adj.groupby('WORD')['WORD'].count().reset_index(name='count').\\\n",
        "    sort_values(['count'], ascending=False).head(15).plot(kind='bar', x='WORD')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh_oAFVKgT6v"
      },
      "source": [
        "### Review Text - Bigram Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkZr_oe2huzu"
      },
      "source": [
        "Bigrams & Trigram Analysis: We want to identify bigrams and trigrams so we can concatenate them and consider them as one word. Bigrams are phrases containing 2 words e.g. ‘social media’, where ‘social’ and ‘media’ are more likely to co-occur rather than appear separately. Likewise, trigrams are phrases containing 3 words that more likely co-occur e.g. ‘Proctor and Gamble’. We use Pointwise Mutual Information score to identify significant bigrams and trigrams to concatenate. We also filter bigrams or trigrams with the filter (noun/adj, noun), (noun/adj,all types,noun/adj) because these are common structures pointing out noun-type n-grams. This helps the LDA model better cluster topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKIjOh0NXLsG"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def get_bigram(text):\n",
        "    token = word_tokenize(text)\n",
        "    bigram = list(ngrams(token, 2))\n",
        "    return bigram\n",
        "\n",
        "\n",
        "review_df['bigram_list'] = review_df['processed_review_detokenized'].apply(lambda x: get_bigram(x))\n",
        "# review_df['bigram_list'].apply(pd.Series).stack().reset_index(drop = True)\n",
        "\n",
        "counter = Counter(review_df['bigram_list'].apply(pd.Series).stack().reset_index(drop = True))\n",
        "counter.most_common(25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Krld8DlgYCR"
      },
      "source": [
        "### Review Text - Trigram Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIgC3qEIgDNO"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def get_trigram(text):\n",
        "    token = word_tokenize(text)\n",
        "    bigram = list(ngrams(token, 3))\n",
        "    return bigram\n",
        "\n",
        "\n",
        "review_df['trigram_list'] = review_df['processed_review_detokenized'].apply(lambda x: get_trigram(x))\n",
        "# review_df['bigram_list'].apply(pd.Series).stack().reset_index(drop = True)\n",
        "\n",
        "counter = Counter(review_df['trigram_list'].apply(pd.Series).stack().reset_index(drop = True))\n",
        "counter.most_common(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qX1v0ue_iFnn"
      },
      "outputs": [],
      "source": [
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "finder = nltk.collocations.BigramCollocationFinder.from_documents([word for word in review_df.loc[:,\"processed_review_tokens\"]])\n",
        "\n",
        "# Filter only those that occur at least 50 times\n",
        "finder.apply_freq_filter(30)\n",
        "bigram_scores = finder.score_ngrams(bigram_measures.pmi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQNXEs7PikVO"
      },
      "outputs": [],
      "source": [
        "bigram_scores[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIAxa3OljKRY"
      },
      "outputs": [],
      "source": [
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
        "finder = nltk.collocations.TrigramCollocationFinder.from_documents([word for word in review_df.loc[:,\"processed_review_tokens\"]])\n",
        "\n",
        "# Filter only those that occur at least 50 times\n",
        "finder.apply_freq_filter(30)\n",
        "trigram_scores = finder.score_ngrams(trigram_measures.pmi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZxDS7y1jVrD"
      },
      "outputs": [],
      "source": [
        "trigram_scores[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVbOt2isjcfF"
      },
      "outputs": [],
      "source": [
        "bigram_pmi = pd.DataFrame(bigram_scores)\n",
        "\n",
        "bigram_pmi.columns = ['bigram', 'pmi']\n",
        "bigram_pmi.sort_values(by='pmi', axis = 0, ascending = False, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o67fkJakjhyE"
      },
      "outputs": [],
      "source": [
        "trigram_pmi = pd.DataFrame(trigram_scores)\n",
        "\n",
        "trigram_pmi.columns = ['trigram', 'pmi']\n",
        "trigram_pmi.sort_values(by='pmi', axis = 0, ascending = False, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTnjEeK2lWkC"
      },
      "outputs": [],
      "source": [
        "trigram_pmi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrB9j5ZXjmWh"
      },
      "outputs": [],
      "source": [
        "# Filter for bigrams with only noun-type structures\n",
        "def bigram_filter(bigram):\n",
        "    tag = nltk.pos_tag(bigram)\n",
        "    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['NN']:\n",
        "        return False\n",
        "    if bigram[0] in stopwords.words('english') or bigram[1] in stopwords.words('english'):\n",
        "        return False\n",
        "    if 'n' in bigram or 't' in bigram:\n",
        "        return False\n",
        "    if 'PRON' in bigram:\n",
        "        return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y0eaeMUjoLP"
      },
      "outputs": [],
      "source": [
        "# Filter for trigrams with only noun-type structures\n",
        "def trigram_filter(trigram):\n",
        "    tag = nltk.pos_tag(trigram)\n",
        "    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['JJ','NN']:\n",
        "        return False\n",
        "    if trigram[0] in stopwords.words('english') or trigram[-1] in stopwords.words('english') or trigram[1] in stopwords.words('english'):\n",
        "        return False\n",
        "    if 'n' in trigram or 't' in trigram:\n",
        "         return False\n",
        "    if 'PRON' in trigram:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE0NNn1Zjz1y"
      },
      "outputs": [],
      "source": [
        "# Need to set pmi threshold to whatever makes sense - eyeball through and select threshold where n-grams stop making sense\n",
        "# choose top 500 ngrams in this case ranked by PMI that have noun like structures\n",
        "\n",
        "filtered_bigram = bigram_pmi[bigram_pmi.apply(lambda bigram: bigram_filter(bigram['bigram']) and bigram.pmi > 4, axis = 1)][:500]\n",
        "# filtered_trigram = trigram_pmi[trigram_pmi.apply(lambda trigram: trigram_filter(trigram['trigram']) and trigram.pmi > 5, axis = 1)][:500]\n",
        "\n",
        "\n",
        "bigrams = [' '.join(x) for x in filtered_bigram.bigram.values if len(x[0]) > 2 or len(x[1]) > 2]\n",
        "# trigrams = [' '.join(x) for x in filtered_trigram.trigram.values if len(x[0]) > 2 or len(x[1]) > 2 and len(x[2]) > 2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnwwPrP6nPa8"
      },
      "outputs": [],
      "source": [
        "bigrams[:25]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnr-y3BToI7t"
      },
      "source": [
        "## **Sentiment Analysis Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_VuyN_IoYq9"
      },
      "source": [
        "### Using CountVectorizer & LogisticsRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH2iQ4_EeTHx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "reviews_X = review_df[\"processed_review_detokenized\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9-8YLBpl04c"
      },
      "outputs": [],
      "source": [
        "y = review_df.review_sentiment.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6GDZx2EjNie"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "lr_st = time.time()\n",
        "\n",
        "cv = CountVectorizer()\n",
        "\n",
        "X = cv.fit_transform(reviews_X)\n",
        "len(cv.get_feature_names())\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.30, random_state=5)\n",
        "\n",
        "lr_model = LogisticRegression()\n",
        "\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = lr_model.predict(X_test)\n",
        "\n",
        "lr_et = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn9SUTD5jtoo"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G87j5TL5s5pD"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65D1VOMn38du"
      },
      "outputs": [],
      "source": [
        "(lr_et - lr_st) * 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PJ0AufX1D2o"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "color = 'white'\n",
        "matrix = plot_confusion_matrix(lr_model, X_test, y_test, cmap=plt.cm.Blues)\n",
        "\n",
        "matrix.ax_.set_title('Confusion Matrix', color=color)\n",
        "plt.xlabel('Predicted Label', color=color)\n",
        "plt.ylabel('True Label', color=color)\n",
        "plt.gcf().axes[0].tick_params(colors=color)\n",
        "plt.gcf().axes[1].tick_params(colors=color)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOtBTxWLkRhN"
      },
      "source": [
        "### Using TfidfVectorizer & MultinomialNB Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY5CqOLukQ-d"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# X = review_df.loc[:,\"review_x\"]\n",
        "X = review_df[\"processed_review_detokenized\"].values\n",
        "y = review_df.loc[:,\"review_sentiment\"]\n",
        "\n",
        "mnb_st = time.time()\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# td = TfidfVectorizer(max_features = 4500)\n",
        "\n",
        "td = TfidfVectorizer(max_features = 4500)\n",
        "\n",
        "X = td.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# Training Classifier and Predicting on Test Data\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB()\n",
        "\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "mnb_et = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW-iDdNFpZXq"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN5zwnFpr2m9"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB2R18J_1mww"
      },
      "outputs": [],
      "source": [
        "color = 'white'\n",
        "matrix = plot_confusion_matrix(classifier, X_test, y_test, cmap=plt.cm.Blues)\n",
        "matrix.ax_.set_title('Confusion Matrix', color=color)\n",
        "plt.xlabel('Predicted Label', color=color)\n",
        "plt.ylabel('True Label', color=color)\n",
        "plt.gcf().axes[0].tick_params(colors=color)\n",
        "plt.gcf().axes[1].tick_params(colors=color)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAwsvWr33Zs4"
      },
      "outputs": [],
      "source": [
        "(mnb_et - mnb_st) * 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxBS5eJ7t_ng"
      },
      "source": [
        "### Using TfidfVectorizer & XGBoost Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7L3YIKMluBt_"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import time\n",
        "\n",
        "xgb_st = time.time()\n",
        "\n",
        "td = TfidfVectorizer(max_features = 4500)\n",
        "\n",
        "reviews_X = review_df[\"processed_review_detokenized\"].values\n",
        "X = td.fit_transform(reviews_X)\n",
        "y = review_df.review_sentiment.values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.30, random_state=5)\n",
        "\n",
        "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(X_train, y_train)\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "xgb_et = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEN-arB2vZj_"
      },
      "outputs": [],
      "source": [
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HdW3yUf2UnW"
      },
      "outputs": [],
      "source": [
        "# Execution Time\n",
        "(xgb_et - xgb_st) * 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syV_VTK4vb0u"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0kTxJmA1--x"
      },
      "outputs": [],
      "source": [
        "color = 'white'\n",
        "matrix = plot_confusion_matrix(xgb_model, X_test, y_test, cmap=plt.cm.Blues)\n",
        "matrix.ax_.set_title('Confusion Matrix', color=color)\n",
        "plt.xlabel('Predicted Label', color=color)\n",
        "plt.ylabel('True Label', color=color)\n",
        "plt.gcf().axes[0].tick_params(colors=color)\n",
        "plt.gcf().axes[1].tick_params(colors=color)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVF9sLEN557V"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "filename = 'sentiment_classification_model.sav'\n",
        "pickle.dump(classifier, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMaNvWTJ6P8R"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2zOuPa3oma5"
      },
      "source": [
        "## **Topic Modelling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfn4QZ5Notkb"
      },
      "source": [
        "Topic modeling is an unsupervised machine learning technique that’s capable of scanning a set of documents, detecting word and phrase patterns within them, and automatically clustering word groups and similar expressions that best characterize a set of documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N23iKH2NpMLU"
      },
      "outputs": [],
      "source": [
        "# Filter for only nouns\n",
        "def noun_only(x):\n",
        "    pos_comment = nltk.pos_tag(x)\n",
        "    filtered = [word[0] for word in pos_comment if word[1] in ['NN']]\n",
        "    # to filter both noun and verbs\n",
        "    #filtered = [word[0] for word in pos_comment if word[1] in ['NN','VB', 'VBD', 'VBG', 'VBN', 'VBZ']]\n",
        "    return filtered\n",
        "\n",
        "# Concatenate n-grams\n",
        "def replace_ngram(x):\n",
        "    # for gram in trigrams:\n",
        "    #     x = x.replace(gram, '_'.join(gram.split()))\n",
        "    for gram in bigrams:\n",
        "        x = x.replace(gram, '_'.join(gram.split()))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hKHZlE0sy26"
      },
      "outputs": [],
      "source": [
        "review_df['processed_review_detokenized'] = review_df['processed_review_detokenized'].map(lambda x: replace_ngram(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxkO61t4sGKg"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "review_df['processed_review_detokenized_tokenized'] = review_df['processed_review_detokenized'].apply(lambda x: word_tokenize(x))\n",
        "review_df['processed_review_detokenized_tokenized'] = review_df['processed_review_detokenized_tokenized'].apply(lambda x: noun_only(x))\n",
        "\n",
        "review_df['processed_review_detokenized_tokenized'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCEDJbFDtRRz"
      },
      "outputs": [],
      "source": [
        "review_df['processed_review_detokenized_tokenized'].values[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e567GnxAtf4K"
      },
      "outputs": [],
      "source": [
        "review_df['processed_review_detokenized'] = review_df.processed_review_detokenized_tokenized.apply(lambda x: token_to_sentence(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6d0g0UsossC"
      },
      "outputs": [],
      "source": [
        "## Topic Modelling using LDA - Latent Diriclet Analysis\n",
        "## Topic Modelling using LSA - Latent Semantic Analysis\n",
        "## Topic Modelling using NMF - Non-negative Matrix Factorization\n",
        "## Topic Modelling using BertTopic - Transformer Models\n",
        "\n",
        "documents = review_df.loc[:,\"processed_review_detokenized\"]\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "no_features = 4000\n",
        "\n",
        "# NMF uses tf-idf\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words=\"english\")\n",
        "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XRvM957O8fn"
      },
      "source": [
        "Checking Sparscity - Sparsicity is nothing but the percentage of non-zero datapoints in the document-word matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjjNMk0-Nu1G"
      },
      "outputs": [],
      "source": [
        "# Checking Sparsity of the TF-IDF Vectors\n",
        "\n",
        "# Materialize the sparse data\n",
        "data_dense = tfidf.todense()\n",
        "\n",
        "# Compute Sparsicity = Percentage of Non-Zero cells\n",
        "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz4tUEbZq9iy"
      },
      "outputs": [],
      "source": [
        "# LDA uses raw term counts for LDA because its a probabilistic graphical model\n",
        "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words=\"english\")\n",
        "tf = tf_vectorizer.fit_transform(documents)\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooJN6DN2O133"
      },
      "source": [
        "Checking Sparscity - Sparsicity is nothing but the percentage of non-zero datapoints in the document-word matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOzwa9nwOctQ"
      },
      "outputs": [],
      "source": [
        "# Checking Sparsity of the CountVectorizer Vectors\n",
        "\n",
        "# Materialize the sparse data\n",
        "data_dense_v = tf.todense()\n",
        "\n",
        "# Compute Sparsicity = Percentage of Non-Zero cells\n",
        "print(\"Sparsicity: \", ((data_dense_v > 0).sum()/data_dense_v.size)*100, \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iENEBlOqrqmq"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
        "\n",
        "no_topics = 5\n",
        "\n",
        "# Run LSA\n",
        "# SVD represent documents and terms in vectors\n",
        "lsa = TruncatedSVD(\n",
        "    n_components=no_topics,\n",
        "    algorithm='randomized',\n",
        "    n_iter=100,\n",
        "    random_state=122).fit(tfidf)\n",
        "\n",
        "# Run NMF\n",
        "nmf = NMF(\n",
        "    n_components=no_topics,\n",
        "    random_state=1,\n",
        "    alpha=.1,\n",
        "    l1_ratio=.5,\n",
        "    init='nndsvd').fit(tfidf)\n",
        "\n",
        "# Run LDA\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=no_topics,\n",
        "    learning_method='online',\n",
        "    learning_decay=0.2,\n",
        "    random_state=0).fit(tf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hTR__MCslGq"
      },
      "source": [
        "### **Displaying and Evaluating Topics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncFsOJaFOJZP"
      },
      "source": [
        "We evaluate our topic models using a mixed approach? Highlighted as below:\n",
        "\n",
        "- Observation-based, eg. observing the top ‘n‘ words in a topic.\n",
        "- Interpretation-based, eg. ‘word intrusion’ and ‘topic intrusion’ to identify the words or topics that “don’t belong” in a topic or document.\n",
        "- Quantitative metrics – Perplexity (held out likelihood) and coherence calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtVQo2w9so2y"
      },
      "outputs": [],
      "source": [
        "def display_topics(model, feature_names, no_top_words):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  for index, component in enumerate(model.components_):\n",
        "    zipped = zip(feature_names, component)\n",
        "    top_terms_key = sorted(\n",
        "        zipped,\n",
        "        key=lambda t:t[1],\n",
        "        reverse=True)[:no_top_words]\n",
        "    top_terms_list = list(dict(top_terms_key).keys())\n",
        "    print(\"Topic \" + str(index) + \": \", top_terms_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3l_HRHs6WoZ"
      },
      "outputs": [],
      "source": [
        "display_topics(nmf, tfidf_feature_names, 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVRfp_vg6gJb"
      },
      "outputs": [],
      "source": [
        "display_topics(lda, tf_feature_names, 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-gWVoZYCMoc"
      },
      "outputs": [],
      "source": [
        "display_topics(lsa, tfidf_feature_names, 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IQgnqK6DMMq"
      },
      "outputs": [],
      "source": [
        "# Model Evaluation\n",
        "# Model Evaluation Strategies\n",
        "# 1. Eye Balling Models - Top N words, Topics / Documents\n",
        "# 2. Intrinsic Evaluation Metrics - Capturing model semantics, Topics interpretability\n",
        "# 3. Human Judgements - What is a topic?\n",
        "# 4. Extrinsic Evaluation Metrics / Evaluation at task.\n",
        "print(\"Log Likelihood, \", lda.score(tf))\n",
        "print(\"Perplexity,\", lda.perplexity(tf)) # Optimizing for perplexity may not yield human intepretable results.\n",
        "print(\"LDA Model Params\", lda.get_params())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUmrN8lgFlBt"
      },
      "outputs": [],
      "source": [
        "from gensim.models import CoherenceModel\n",
        "import gensim.corpora as corpora\n",
        "\n",
        "def get_coherence_value(model, df_column, n_top_words):\n",
        "    topics = model.components_\n",
        "    n_top_words = n_top_words\n",
        "    texts = [[word for word in doc.split()] for doc in df_column]\n",
        "\n",
        "    # Create the dictionary\n",
        "    dictionary = corpora.Dictionary(texts)\n",
        "\n",
        "    # Create a gensim corpus from the word count matrix\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "    feature_names = [dictionary[i] for i in range(len(dictionary))]\n",
        "\n",
        "    # Get the top words for each topic from the components_ attribute\n",
        "    top_words = []\n",
        "    for topic in topics:\n",
        "      top_words.append([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "\n",
        "    coherence_model = CoherenceModel(\n",
        "        topics=top_words,\n",
        "        texts=texts,\n",
        "        dictionary=dictionary,\n",
        "        coherence='c_v')\n",
        "\n",
        "    coherence = coherence_model.get_coherence()\n",
        "    return coherence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi1ix-RjS42n"
      },
      "outputs": [],
      "source": [
        "get_coherence_value(lda, review_df['processed_review_detokenized'], 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8iodGy7HVlS"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-JnSiFZTKE_"
      },
      "outputs": [],
      "source": [
        "print(\"LSA Coherence\", get_coherence_value(lsa, review_df['processed_review_detokenized'], 20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxjVRC5fTYES"
      },
      "outputs": [],
      "source": [
        "get_coherence_value(nmf, review_df['processed_review_detokenized'], 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjmSRFfwdomq"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "dictionary = corpora.Dictionary(review_df.loc[:,\"processed_review_tokens\"])\n",
        "texts = review_df.loc[:,\"processed_review_tokens\"]\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "coherence = []\n",
        "for k in range(1,9):\n",
        "    print('Round: '+ str(k))\n",
        "    Lda = gensim.models.ldamodel.LdaModel\n",
        "    ldamodel = Lda(corpus, num_topics=k, \\\n",
        "               id2word = dictionary, passes=40,\\\n",
        "               iterations=200, chunksize = 10000, eval_every = None)\n",
        "\n",
        "    cm = gensim.models.coherencemodel.CoherenceModel(\\\n",
        "         model=ldamodel, texts=texts,\\\n",
        "         dictionary=dictionary, coherence='c_v')\n",
        "\n",
        "    coherence.append((k,cm.get_coherence()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXYZAl5nkmAw"
      },
      "outputs": [],
      "source": [
        "# coherence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0h0V20YNLEk"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from pprint import pprint\n",
        "# Grid searching to get the best LDA model etc.\n",
        "\n",
        "# Define Search Param\n",
        "search_params = {'n_components': [2, 3, 4, 5, 10], 'learning_decay': [.2, .5, .7, .9]}\n",
        "\n",
        "# Init the Model\n",
        "lda = LatentDirichletAllocation()\n",
        "\n",
        "# Init Grid Search Class\n",
        "model = GridSearchCV(lda, param_grid=search_params)\n",
        "\n",
        "# Do the Grid Search\n",
        "model.fit(tf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWqYzcKtQG-L"
      },
      "outputs": [],
      "source": [
        "%%script echo skipping\n",
        "\n",
        "# Best Model\n",
        "best_lda_model = model.best_estimator_\n",
        "\n",
        "# Model Parameters\n",
        "print(\"Best Model's Params: \", model.best_params_)\n",
        "\n",
        "# Log Likelihood Score\n",
        "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
        "\n",
        "# Perplexity\n",
        "print(\"Model Perplexity: \", best_lda_model.perplexity(tf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAIelvHoJhOr"
      },
      "source": [
        "### Topic Modelling with BertTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UHKciFDI8Ti"
      },
      "outputs": [],
      "source": [
        "!pip -q install bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dRWWx5zKgeO"
      },
      "outputs": [],
      "source": [
        "!pip show tqdm\n",
        "!pip uninstall tqdm\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cxF-6YydwXrY"
      },
      "outputs": [],
      "source": [
        "!pip -q install umap\n",
        "!pip -q install hdbscan\n",
        "!pip -q install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzLUnKT0wIN3"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=100)\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=20, min_samples=40,\n",
        "                        gen_min_span_tree=True,\n",
        "                        prediction_data=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e4kDTkEJEqb"
      },
      "outputs": [],
      "source": [
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords = list(stopwords.words('english')) + ['airline', 'kenya', 'nairobi','flight','hour']\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "# topic_model = BERTopic(language=\"multilingual\")\n",
        "# topics, probs = topic_model.fit_transform(review_df.loc[:,\"processed_review_detokenized\"])\n",
        "# topic_model.update_topics(review_df.loc[:,\"processed_review_detokenized\"], n_gram_range=(1, 3))\n",
        "\n",
        "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=stopwords)\n",
        "\n",
        "seed_topic_list = [\n",
        "    [\"time\", \"delay\", \"airport\",\"time\",  \"hour\"],\n",
        "    [\"baggage\", \"luggage\", \"bag\"],\n",
        "    [\"seat\", \"comfort\", \"cleanliness\",\"legroom\", \"entertainment\", \"space\"],\n",
        "    [\"food\", \"meal\", \"snack\", \"drink\",\"beverage\"],\n",
        "    [\"staff\", \"service\", \"airline staff\", \"service staff\", \"ground_staff\", \"crew\", \"pilot\", \"customer_service\"],\n",
        "    [\"airport\", \"flight\", \"connecting_flight\", \"experience\", \"board\", \"dreamliner\"],\n",
        "    [\"business\", \"business_class\", \"economy\"],\n",
        "    [\"hotel\"]\n",
        "    ]\n",
        "\n",
        "model = BERTopic(\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    embedding_model=embedding_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    top_n_words=10,\n",
        "    language='english',\n",
        "    calculate_probabilities=True,\n",
        "    verbose=True,\n",
        "    diversity=0.2,\n",
        "    seed_topic_list=seed_topic_list\n",
        ")\n",
        "\n",
        "topics, probs = model.fit_transform(review_df.processed_review_detokenized.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLhWPwVjJpYX"
      },
      "outputs": [],
      "source": [
        "model.get_topic_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmiBs_gWMdtI"
      },
      "outputs": [],
      "source": [
        "model.get_representative_docs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO1C-wpAjxwP"
      },
      "outputs": [],
      "source": [
        "# model.visualize_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRSuUEUkJvWb"
      },
      "outputs": [],
      "source": [
        "model.get_topic(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAVqF7i6fVM9"
      },
      "outputs": [],
      "source": [
        "model.get_topic(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2jI9s53fXe_"
      },
      "outputs": [],
      "source": [
        "model.get_topic(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znaAS6GuzRFP"
      },
      "outputs": [],
      "source": [
        "model.get_topic(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpTLL7Id2ixY"
      },
      "outputs": [],
      "source": [
        "model.get_topic(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "154irzCo23iA"
      },
      "outputs": [],
      "source": [
        "model.visualize_barchart(topics=[-1,0,1,2,3], n_words=10, custom_labels=False, width=350, height=350)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1fSn0FwJKXP"
      },
      "outputs": [],
      "source": [
        "model.set_topic_labels({\n",
        "    -1:\"General Flight Experience\",\n",
        "     0:\"Flight Time / Service Time\",\n",
        "     1:\"In-Flight Service\",\n",
        "     2:\"Staff Experience / Delays\",\n",
        "     3:\"Flight Operations (Baggage Handling)\",\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpisz2eTKK0c"
      },
      "outputs": [],
      "source": [
        "model.visualize_barchart(topics=[-1,0,1,2,3], n_words=10, custom_labels=True, width=350, height=350, title=\"Review Topic Classification - KQ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdYPh00t_Wfd"
      },
      "outputs": [],
      "source": [
        "custom_topic_names = [\"In-Flight Service (Food, Entertainment, Meal)\", \"Flight Time (Service Time, Departure, Arrival)\", \"Staff & Crew Experience\", \"Luggage Handling\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2c_WpWUAOWW"
      },
      "outputs": [],
      "source": [
        "model.visualize_term_rank()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEneg4zE9HEQ"
      },
      "outputs": [],
      "source": [
        "model.visualize_hierarchy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO96fW-QEYH0"
      },
      "source": [
        "Intertopic distance map measures the distance between topics. Similar topics are closer to each other, and very different topics are far from each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sl_utcjrAg5K"
      },
      "outputs": [],
      "source": [
        "model.visualize_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhkNDS0JCq2f"
      },
      "outputs": [],
      "source": [
        "model.visualize_heatmap()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QF2blhXDF5C"
      },
      "source": [
        "The topic prediction for a document is based on the predicted probabilities of the document belonging to each topic. The topic with the highest probability is the predicted topic. This probability represents how confident we are about finding the topic in the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16_fUWB1E8Fk"
      },
      "outputs": [],
      "source": [
        "review_df.processed_review_detokenized.values[45]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiaXJBt9C0Nt"
      },
      "outputs": [],
      "source": [
        "model.visualize_distribution(model.probabilities_[45], min_probability=0.015)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH_VIxO4FvTM"
      },
      "outputs": [],
      "source": [
        "embeddings = embedding_model.encode(review_df.processed_review_detokenized.values, show_progress_bar=False)\n",
        "model.visualize_documents(review_df.processed_review_detokenized.values, embeddings=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSw0YH6TG-Tg"
      },
      "outputs": [],
      "source": [
        "# Calculate the topic distributions on a token-level\n",
        "topic_distr, topic_token_distr = model.approximate_distribution(review_df.processed_review_detokenized.values, calculate_tokens=True)\n",
        "\n",
        "# Visualize the token-level distributions\n",
        "df = model.visualize_approximate_distribution(review_df.processed_review_detokenized.values[8], topic_token_distr[8])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miR2xerHDkA6"
      },
      "source": [
        "Saving the Final Topic Model in the Colab Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JC8nzy4XaQ7r"
      },
      "outputs": [],
      "source": [
        "# Save the topic model\n",
        "model.save(\"kq_review_topic_model\")\n",
        "# Load the topic model\n",
        "model = BERTopic.load(\"kq_review_topic_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dP_gkgJOMW7F"
      },
      "outputs": [],
      "source": [
        "review_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_4mlboiNnmU"
      },
      "outputs": [],
      "source": [
        "review_df[\"Review Topic\"] = topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiA_-yGxOWrH"
      },
      "outputs": [],
      "source": [
        "topic_1 = []\n",
        "topic_2 = []\n",
        "topic_3 = []\n",
        "topic_4 = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnPqlzz-N0GF"
      },
      "outputs": [],
      "source": [
        "for i in probs:\n",
        "  topic_1.append(i[0])\n",
        "  topic_2.append(i[1])\n",
        "  topic_3.append(i[2])\n",
        "  topic_4.append(i[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_jDpSfPO_WF"
      },
      "outputs": [],
      "source": [
        "review_df[\"topic_1_prob\"] = topic_1\n",
        "review_df[\"topic_2_prob\"] = topic_2\n",
        "review_df[\"topic_3_prob\"] = topic_3\n",
        "review_df[\"topic_4_prob\"] = topic_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKf-zny_PNjC"
      },
      "outputs": [],
      "source": [
        "review_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8939xaeyPdvX"
      },
      "outputs": [],
      "source": [
        "review_df.to_csv('final_topical_annotated_dataset.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "j4rYvvQ61Ps7",
        "sII_fRkJpVcT",
        "RdNUoUxq9XNA",
        "aB5iN376RWDo",
        "8WxPPNl0RcRH",
        "8oOLLEXfRfj8",
        "pYf9YG2GRjIL",
        "n6Peyes6Rowq",
        "xjZz7X0gRw9q",
        "kZe6gvL1SO-T",
        "dVmmWr8LSjsC",
        "aFC4ts_yTchJ",
        "znqKrX7ZTtpb",
        "FzY5o-D7T2cU",
        "ET3DkYQyt0rf",
        "wvNVWzd8uk0Z",
        "R5Fe-nXgZ3G7",
        "3KTia-Ad8L6w",
        "YvyCIsjiAseg",
        "6V63NJ4jBlXl",
        "1ZosZNZ4mAvp",
        "zDXZ9KbArND8",
        "clfUWwL0Yxqm",
        "tYsbet3FXjwg",
        "FDt1YXyDc74S",
        "gpXMSwBvsqE0",
        "0CBTTd-Da8RB",
        "E8DIrwWyCFaj",
        "GIFr98omeGvK"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}